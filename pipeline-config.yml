# pipeline-config.yml - Your pipeline configuration
pipeline:
  name: "GenomicRiskPrediction_v2"
  version: "2.1.0"
  environment: "AWS_EMR_Cluster"

data_ingestion:
  method: "Batch Processing via Apache Spark"
  tools: 
    - "Apache Spark 3.3"
    - "Hail 0.2" 
    - "AWS EMR"
    - "Apache Parquet"
  performance:
    throughput: "1.2 TB/hour"
    parallelization: "Chromosome-level partitioning"

storage:
  architecture: "Medallion Lakehouse"
  layers:
    bronze:
      format: "VCF.gz, BAM"
      storage: "AWS S3 Standard"
      retention: "7 years"
    silver:
      format: "Apache Parquet"
      storage: "AWS S3 Intelligent Tiering"
      compression: "Snappy"
    gold:
      format: "Feather, Parquet"
      storage: "AWS S3 One Zone-IA"
    platinum:
      format: "MLflow, PMML"
      storage: "AWS S3 + EFS"

processing:
  data_cleaning:
    - "PySpark for distributed QC"
    - "Hail for genomic operations"
    - "Variant normalization"
  transformation:
    - "Distributed feature engineering"
    - "PRS computation"
    - "LD pruning"

analytics_modeling:
  machine_learning:
    primary: "XGBoost 1.7"
    alternatives: 
      - "Random Forest"
      - "Logistic Regression"
    frameworks:
      - "Scikit-learn"
      - "MLflow"
      - "Hyperopt"

visualization:
  dashboards:
    - "Grafana for pipeline metrics"
    - "Tableau for clinical insights"
  exploration:
    - "Plotly for interactive plots"
    - "Matplotlib for publication figures"
    - "Seaborn for statistical visuals"

cost_estimation:
  total_per_run: "$2,800"
  breakdown:
    ingestion: "$300"
    quality_control: "$576"
    feature_engineering: "$576"
    ml_training: "$48"
    storage: "$315/month"
